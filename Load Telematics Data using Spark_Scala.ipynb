{
    "metadata": {
        "language_info": {
            "name": "scala", 
            "pygments_lexer": "scala", 
            "mimetype": "text/x-scala", 
            "codemirror_mode": "text/x-scala", 
            "file_extension": ".scala", 
            "version": "2.11.8"
        }, 
        "kernelspec": {
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.0", 
            "name": "scala-spark20"
        }
    }, 
    "nbformat_minor": 0, 
    "cells": [
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "// Making REST call using scalaj-http library\n//JSON parsing of REST results using play-json library\n//Other important Imports\n%AddJar -magic http://central.maven.org/maven2/org/scalaj/scalaj-http_2.10/2.3.0/scalaj-http_2.10-2.3.0.jar\n%AddJar -magic http://central.maven.org/maven2/com/typesafe/play/play-json_2.10/2.4.6/play-json_2.10-2.4.6.jar\nimport scalaj.http._\nimport play.api.libs.json._\nimport scala.util.{Try,Success,Failure}", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Using cached version of scalaj-http_2.10-2.3.0.jar\nUsing cached version of play-json_2.10-2.4.6.jar\n"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 1
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "// loading test dataset from Udacity Open Source Sample Data \nval UdacityDataURL = \"https://github.com/smatlapudi/testdrive-bluemix-driverbehavior-service/raw/master/udacity-ch3-north/testdata/udacity-ch3-north.json\"\nval MastikshTestData =  scala.io.Source.fromURL(UdacityDataURL).getLines.to[collection.immutable.Seq]\n//Create SCALA RDD from Udacity sample data \nval tripDataLines_rdd = sc.parallelize(MastikshTestData)\ntripDataLines_rdd.coalesce(1).saveAsTextFile(\"MastikshTestData1\")\n//check sample data to make sure RDD loaded properly\ntripDataLines_rdd.take(5)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Array({\"timestamp\":\"2016-11-17T22:23:03Z\",\"heading\":357.4195341311482,\"speed\":15.085078615161539,\"latitude\":37.4019584656,\"longitude\":-122.114883423}, {\"timestamp\":\"2016-11-17T22:23:04Z\",\"heading\":357.12198437345154,\"speed\":15.001162178540001,\"latitude\":37.4020271301,\"longitude\":-122.115020752}, {\"timestamp\":\"2016-11-17T22:23:05Z\",\"heading\":355.95965463973073,\"speed\":14.618271412569998,\"latitude\":37.4021072388,\"longitude\":-122.115158081}, {\"timestamp\":\"2016-11-17T22:23:06Z\",\"heading\":330.98606173371456,\"speed\":14.17331637527667,\"latitude\":37.4021835327,\"longitude\":-122.115287781}, {\"timestamp\":\"2016-11-17T22:23:07Z\",\"heading\":351.05080395396425,\"speed\":13.715587838680003,\"latitude\":37.4022636414,\"longitude\":-122.115409851})"
                    }, 
                    "output_type": "execute_result", 
                    "execution_count": 2
                }
            ], 
            "cell_type": "code", 
            "execution_count": 2
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SparkSession  \nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\nval session = SparkSession.builder().appName(\"Mastiksh\").master(\"local\").getOrCreate()\ncase class TripDataPoint(timestamp: String, heading: Double, speed: Double, latitude: Double, longitude: Double )\nval jsonSchema = new StructType().add(\"timestamp\", \"string\").add(\"heading\", \"double\").add(\"speed\", \"double\").add(\"latitude\", \"double\").add(\"longitude\", \"double\")\n// Convert tripDataLines_rdd to TripRdd \nval streamingInputDF = session.read.schema(jsonSchema).json(tripDataLines_rdd)\nstreamingInputDF.show(5)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------------------+------------------+------------------+-------------+--------------+\n|           timestamp|           heading|             speed|     latitude|     longitude|\n+--------------------+------------------+------------------+-------------+--------------+\n|2016-11-17T22:23:03Z| 357.4195341311482|15.085078615161539|37.4019584656|-122.114883423|\n|2016-11-17T22:23:04Z|357.12198437345154|15.001162178540001|37.4020271301|-122.115020752|\n|2016-11-17T22:23:05Z|355.95965463973073|14.618271412569998|37.4021072388|-122.115158081|\n|2016-11-17T22:23:06Z|330.98606173371456| 14.17331637527667|37.4021835327|-122.115287781|\n|2016-11-17T22:23:07Z|351.05080395396425|13.715587838680003|37.4022636414|-122.115409851|\n+--------------------+------------------+------------------+-------------+--------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 42
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "//Check Schema\nstreamingInputDF.printSchema()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- timestamp: string (nullable = true)\n |-- heading: double (nullable = true)\n |-- speed: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n\n"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 43
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "//Register the DataFrame as a table so that you can run SQL queries on it and show the first two rows:\nstreamingInputDF.registerTempTable(\"Mastiksh_Telematics\")\nval Mastiksh_Table1 = session.sql(\"SELECT * FROM Mastiksh_Telematics limit\")\nMastiksh_Table1.show(5)\nMastiksh_Table1.count()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------------------+------------------+------------------+-------------+--------------+\n|           timestamp|           heading|             speed|     latitude|     longitude|\n+--------------------+------------------+------------------+-------------+--------------+\n|2016-11-17T22:23:03Z| 357.4195341311482|15.085078615161539|37.4019584656|-122.114883423|\n|2016-11-17T22:23:04Z|357.12198437345154|15.001162178540001|37.4020271301|-122.115020752|\n|2016-11-17T22:23:05Z|355.95965463973073|14.618271412569998|37.4021072388|-122.115158081|\n|2016-11-17T22:23:06Z|330.98606173371456| 14.17331637527667|37.4021835327|-122.115287781|\n|2016-11-17T22:23:07Z|351.05080395396425|13.715587838680003|37.4022636414|-122.115409851|\n+--------------------+------------------+------------------+-------------+--------------+\nonly showing top 5 rows\n\n"
                }, 
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "2767"
                    }, 
                    "output_type": "execute_result", 
                    "execution_count": 29
                }
            ], 
            "cell_type": "code", 
            "execution_count": 29
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "// loading test dataset from Udacity Open Source Sample Data \nval UdacityDRURL = \"https://raw.githubusercontent.com/smatlapudi/testdrive-bluemix-driverbehavior-service/master/udacity-ch3-north/result/udacity-ch3-north-driverBehaviorResult.json\"\nval MastikshTestDRData =  scala.io.Source.fromURL(UdacityDRURL).getLines.to[collection.immutable.Seq]\n//Create SCALA RDD from Udacity sample data \nval tripDRLines_rdd = sc.parallelize(MastikshTestDRData)\ntripDRLines_rdd.coalesce(1).saveAsTextFile(\"MastikshTestDRData3\")\n//check sample data to make sure RDD loaded properly\ntripDRLines_rdd.take(20)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Array({, \"  \"id\" : {\", \"    \"trip_uuid\" : \"7fbdc6af-179b-4346-9f2d-65d382f30429\",\", \"    \"tenant_id\" : \"c97244f8-e5b1-4982-94c4-0b4bd5c5b711\"\", \"  },\", \"  \"end_altitude\" : 0.0,\", \"  \"end_latitude\" : 37.5503832582108,\", \"  \"end_longitude\" : -122.3120112947789,\", \"  \"end_time\" : 1479424149000,\", \"  \"generated_time\" : 1480987518794,\", \"  \"mo_id\" : \"udacity-ch3\",\", \"  \"driver_id\" : \"\",\", \"  \"start_altitude\" : 0.0,\", \"  \"start_latitude\" : 37.4019766,\", \"  \"start_longitude\" : -122.114744,\", \"  \"start_time\" : 1479421383000,\", \"  \"trip_id\" : \"udacity-ch3-north\",\", \"  \"ctx_sub_trips\" : [ {\", \"    \"id\" : {\", \"      \"sub_trip_id\" : \"d4f784c9-21b1-46cd-8560-e218b1e3853b\",\")"
                    }, 
                    "output_type": "execute_result", 
                    "execution_count": 47
                }
            ], 
            "cell_type": "code", 
            "execution_count": 47
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "//Create an Apache\u00ae Spark machine learning model\n//Prepare data : split your data into: train, test and predict data sets\nval splits = streamingInputDF.randomSplit(Array(0.8, 0.18, 0.02), seed = 24L)\nval Mks_training_data = splits(0).cache()\nval Mks_test_data = splits(1)\nval Mks_prediction_data = splits(2)\n\nprintln(\"Number of training records: \", Mks_training_data.count())\nprintln(\"Number of testing records : \", Mks_test_data.count())\nprintln(\"Number of prediction records : \", Mks_prediction_data.count())", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "(Number of training records: ,2236)\n(Number of testing records : ,482)\n(Number of prediction records : ,49)\n"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 32
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "// Spark ML libraries\nimport org.apache.spark.ml.classification.RandomForestClassifier\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, IndexToString, VectorAssembler}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\nimport org.apache.spark.sql.SparkSession", 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 39
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }
    ], 
    "nbformat": 4
}